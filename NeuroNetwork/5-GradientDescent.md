# æ¢¯åº¦ä¸‹é™ Gradient Descent

``é˜¿æ‰£``ï¼šé˜¿ç‰¹ï¼Œè¿˜è®°å¾—è®­ç»ƒç¥žç»ç½‘ç»œçš„ç›®æ ‡å…¶å®žæ˜¯ä»€ä¹ˆå—ï¼Ÿ

``é˜¿ç‰¹``ï¼šæˆ‘è®°å¾—å¥½åƒæ˜¯è¦æ‰¾å‡ºæœ€åˆé€‚çš„æƒé‡(weights)ï¼Œä½¿å¾—è¾“å‡ºç»“æžœå°½å¯èƒ½æŽ¥è¿‘çœŸå®žå€¼ã€‚

``é˜¿æ‰£``ï¼šæ£’æ£’ï¼æ˜¯è¿™æ ·çš„ã€‚ä¸ºäº†æ‰¾åˆ°è¿™äº›æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä¸€ä¸ªé‡è¦çš„æ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™ã€‚

``é˜¿ç‰¹``ï¼šå¬èµ·æ¥åƒåæ»‘æ»‘æ¢¯~

``é˜¿æ‰£``ï¼šæ˜¯æœ‰é‚£ä¹ˆç‚¹æ„æ€ã€‚è¯´å›žåˆ°è®­ç»ƒç¥žç»ç½‘ç»œï¼Œæˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒä¸­åŠæ—¶äº†è§£è®­ç»ƒæ•ˆæžœå¦‚ä½•ï¼Œæ˜¯ä¸æ˜¯æœç€è®­ç»ƒç›®æ ‡åœ¨ä¸€ç‚¹ç‚¹é è¿‘ã€‚å¦‚æžœåç¦»ç›®æ ‡ï¼Œå°±è¯´æ˜Žè®­ç»ƒæ¨¡åž‹å¯èƒ½åœ¨ã€ŒçŠ¯é”™ã€ï¼Œå°±è¦çº æ­£è¿‡æ¥ã€‚

``é˜¿ç‰¹``ï¼šé‚£æ€Žä¹ˆçŸ¥é“æ¨¡åž‹æ˜¯ä¸æ˜¯åœ¨ã€ŒçŠ¯é”™ã€å‘¢ï¼Ÿ

``é˜¿æ‰£``ï¼šæˆ‘ä»¬ä¼šæ‰¾ä¸€ä¸ªåº¦é‡æ ‡å‡†ã€‚ä¸€ä¸ªå¸¸è§çš„åº¦é‡æ˜¯è¯¯å·®çš„å¹³æ–¹å’Œï¼ˆSSE, sum of the squared errorsï¼‰ï¼š

$$ E=\frac{1}{2}\sum_\mu\sum_j[y^\mu_j - f(\sum_i w_{ij}x^\mu_i)]^2 $$

``é˜¿ç‰¹``ï¼šä½ â€¦â€¦æ¬ºè´Ÿäºº >.<

``é˜¿æ‰£``ï¼šåˆ«ç€æ€¥ï¼Œæˆ‘ä»¬æ¥æ‹†è§£è¿™ä¸€å¨æ˜¯ä¸ªä»€ä¹ˆä¸œè¥¿ã€‚å…ˆçœ‹çœ‹å„ä¸ªå­—æ¯çš„å«ä¹‰ï¼š

![](http://7xjpra.com1.z0.glb.clouddn.com/il_for_SSE-1.png)

è¿™ä¸ªç­‰å¼é‡Œé¢ï¼Œæœ‰ä¸‰ä¸ªæ±‚å’Œé¡¹ï¼Œå°±æ˜¯è¿™ä¸ªç¿»è½¬äº† 90Â° çš„ Mï¼š $\sum$ ã€‚

æœ€å³è¾¹çš„æ±‚å’Œ $\sum_i w_{ij}x^\mu_i$ è¡¨ç¤ºæˆ‘ä»¬è®­ç»ƒå‡ºæ¥çš„æƒé‡ï¼Œæ ¹æ®è¾“å…¥å€¼ x å¾—å‡ºçš„ç›®æ ‡å€¼ $\hat y$ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬ç»™æ•°æ®æ‰“ç®—çš„æ ‡ç­¾ï¼‰ï¼Œç„¶åŽç”¨è¿™äº›ç»“æžœè·Ÿå®žé™…çš„æ•°æ®ä¸­çš„ y å€¼åšæ¯”è¾ƒçœ‹çœ‹åå·®æœ‰å¤šå¤§ã€‚

çŽ°åœ¨ä½ ç†è§£äº†æœ€å³è¾¹çš„æ±‚å’Œé¡¹äº†å—ï¼Ÿ

``é˜¿ç‰¹``ï¼šå¤§æ¦‚æ„æ€æ˜¯æˆ‘ä»¬ä»Žæ•°æ®ä¸­é¢„æµ‹å‡ºæ¥çš„ y ï¼Ÿ

``é˜¿æ‰£``ï¼šæ²¡é”™ï¼Œæˆ‘ä»¬å…ˆæŠŠè¿™ä¸€å¨æ›¿æ¢æˆ $\hat y$ï¼Œç®€åŒ–ä¸€ä¸‹å…¬å¼ï¼š

$$
E=\frac{1}{2}\sum_\mu\sum_j[y^\mu_j - f(\sum_i w_{ij}x^\mu_i)]^2
\\
\downarrow
\\
E=\frac{1}{2}\sum_\mu\sum_j[y^\mu_j - \hat y_j]^2
$$

``é˜¿ç‰¹``ï¼šä¸–ç•Œæ¸…é™å¤šäº†~

``é˜¿æ‰£``ï¼šæˆ‘ä»¬å†æ¥çœ‹å³è¾¹è¿™ä¸ªæ±‚å’Œé¡¹ã€‚j è¡¨ç¤ºæœ‰ j ä¸ªéšå±‚èŠ‚ç‚¹ï¼ŒæŠŠæ¯ä¸ªèŠ‚ç‚¹çš„è¯¯å·®å¹³æ–¹ $[y^\mu_j - \hat y_j]$ è®¡ç®—å‡ºæ¥ã€‚æœ€åŽä¸€ä¸ªæ±‚å’Œé¡¹å°±ç®€å•äº†ï¼Œå®ƒè¡¨ç¤ºæŠŠ u ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„è¯¯å·®åŠ èµ·æ¥ã€‚è¿™æ ·å°±å¾—åˆ°äº†æ€»ä½“è¯¯å·®ã€‚

``é˜¿ç‰¹``ï¼šå¥½äº†ï¼ŒçŸ¥é“äº†è¯¯å·®ï¼Œç„¶åŽå‘¢ï¼Ÿ

``é˜¿æ‰£``ï¼šè®°å¾—æˆ‘ä»¬çš„ç›®çš„æ˜¯æ‰¾å‡ºèƒ½å¤Ÿè®©è¯¯å·®å€¼ï¼ˆSSEï¼‰æœ€å°çš„æƒé‡ $w_{ij}$ ã€‚ä¸‹é¢æœ‰è¯·ã€Œæ¢¯åº¦ä¸‹é™ã€ Gradient Descentã€‚

``é˜¿ç‰¹``ï¼šç»ˆäºŽèƒ½åæ»‘æ»‘æ¢¯äº†â€¦â€¦

``é˜¿æ‰£``ï¼šåè¿™ä¸ªæ»‘æ»‘æ¢¯å¯èƒ½æœ‰ç‚¹æ™• ðŸ˜„ ã€‚æ‰€è°“ã€Œæ¢¯åº¦ã€å…¶å®žæ˜¯å¤šå˜é‡å‡½æ•°çš„å¯¼æ•°ã€‚

``é˜¿ç‰¹``ï¼šå¯¼æ•°ï¼Ÿï¼ä½ è¯´çš„æ˜¯å¾®ç§¯åˆ†é‡Œé¢é‚£ä¸ªå¯¼æ•°å—ï¼Ÿ â€¦â€¦ ç‘Ÿç‘Ÿå‘æŠ–.gif

``é˜¿æ‰£``ï¼šåˆ«ç´§å¼ ï¼Œå…ˆå¬æˆ‘è®²ï¼Œå›žå¿†å›žå¿†ã€‚

``é˜¿ç‰¹``ï¼šå¥½å§

``é˜¿æ‰£``ï¼šä½ è¿˜è®°å¾—æ€Žä¹ˆè¡¨ç¤ºå‡½æ•° f(x) çš„å¯¼æ•°å§ï¼Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯ f'(x) ã€‚

``é˜¿ç‰¹``ï¼šå—¯å—¯ï¼Œè®°å¾—ã€‚

``é˜¿æ‰£``ï¼šæˆ‘ä»¬å¼•å…¥ã€Œè¯¯å·®é¡¹ã€$\delta$ ï¼Œå®ƒè¡¨ç¤º ``è¯¯å·® * æ¿€æ´»å‡½æ•°çš„å¯¼æ•°``



![](http://7xjpra.com1.z0.glb.clouddn.com/WX20171127-154242@2x.png)

![](http://img.blog.csdn.net/20170722164242197)

```python
# Defining the sigmoid function for activations
def sigmoid(x):
    return 1/(1+np.exp(-x))

# Derivative of the sigmoid function
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Input data
x = np.array([0.1, 0.3])
# Target
y = 0.2
# Input to output weights
weights = np.array([-0.8, 0.5])

# The learning rate, eta in the weight step equation
learnrate = 0.5

# the linear combination performed by the node (h in f(h) and f'(h))
h = x[0]*weights[0] + x[1]*weights[1]
# or h = np.dot(x, weights)

# The neural network output (y-hat)
nn_output = sigmoid(h)

# output error (y - y-hat)
error = y - nn_output

# output gradient (f'(h))
output_grad = sigmoid_prime(h)

# error term (lowercase delta)
error_term = error * output_grad

# Gradient descent step
del_w = [ learnrate * error_term * x[0],
          learnrate * error_term * x[1]]
# or del_w = learnrate * error_term * x
```

### Stochastic Gradient Descent

Compute the average loss for a very small random fraction of the training data. SGD scales well both data and model size.

for SGD

- inputs: mean = 0, small equal variance
- initial weights: random, mean = 0, small equal variance

å¦‚ä½•æ”¹å–„ SGD æ•ˆæžœ

- Momentum
  We can take advantage of the knowledge that we've accumulated from previous steps about where we should be headed.
  ![](http://7xjpra.com1.z0.glb.clouddn.com/Momentum.png)
- Learning Rate Decay
  take smaller, noisier steps towards objective. make that step smaller and smaller as you train.
  ![](http://7xjpra.com1.z0.glb.clouddn.com/Learning%20Rate%20Decay.png)
- ADAGRAD

### Mini-batching

å°æ‰¹é‡å¯¹æ•°æ®é›†çš„å­é›†è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯ä¸€æ¬¡å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¿™è®©æˆ‘ä»¬å³ä½¿åœ¨ç¼ºä¹å­˜å‚¨æ•´ä¸ªæ•°æ®é›†çš„å†…å­˜æ—¶ä¹Ÿå¯ä»¥è®­ç»ƒã€‚å®ƒè·Ÿ SGD ç»“åˆçš„æ•ˆæžœæ›´å¥½ã€‚

åœ¨æ¯ä¸ª epoch çš„å¼€å§‹ shuffle æ•°æ®ï¼Œç„¶åŽåˆ›å»ºå°æ‰¹é‡ã€‚å¯¹äºŽæ¯ä¸ªå°æ‰¹é‡ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥è®­ç»ƒç½‘ç»œæƒé‡ã€‚ç”±äºŽè¿™äº›æ‰¹æ¬¡æ˜¯éšæœºçš„ï¼Œå› æ­¤æ¯ä¸ªæ‰¹æ¬¡éƒ½æ‰§è¡ŒSGDã€‚

```python
# Features and Labels
features = tf.placeholder(tf.float32, [None, n_input])
labels = tf.placeholder(tf.float32, [None, n_classes])
```
None æ˜¯ batch size çš„ placeholder

### Epochs

epoch æ˜¯æ•´ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªå‘å‰å’Œå‘åŽä¼ é€’ï¼Œç”¨äºŽå¢žåŠ æ¨¡åž‹çš„å‡†ç¡®æ€§è€Œä¸éœ€è¦æ›´å¤šçš„æ•°æ®ã€‚

- [Gradient Descent with Squared Errors](https://classroom.udacity.com/nanodegrees/nd101-cn/parts/ba124b66-b7f7-43ab-bc89-a390adb57f92/modules/2afd43e6-f4ce-4849-bde6-49d7164da71b/lessons/dc37fa92-75fd-4d41-b23e-9659dde80866/concepts/7d480208-0453-4457-97c3-56c720c23a89)
- [Gradient (video) | Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)
- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html#momentum)
