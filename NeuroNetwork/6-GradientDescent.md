# æ¢¯åº¦ä¸‹é™ Gradient Descent

``é˜¿æ‰£``ï¼šä¸Šä¸€æ¬¡æˆ‘ä»¬äº†è§£äº†æŸå¤±å‡½æ•°ã€‚ä¸ºäº†æ‰¾åˆ°ä½¿å¾—æŸå¤±å‡½æ•°ï¼ˆæ¯”å¦‚ç”¨ SSE è®¡ç®—è¯¯å·®ï¼‰æœ€å°çš„ w (æƒé‡) å’Œ b (åç½®é¡¹)ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä¸€ä¸ªé‡è¦çš„æ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™ã€‚

``é˜¿ç‰¹``ï¼šå¬èµ·æ¥åƒåæ»‘æ»‘æ¢¯~

``é˜¿æ‰£``ï¼šæ˜¯æœ‰é‚£ä¹ˆç‚¹æ„æ€ã€‚

``é˜¿æ‰£``ï¼šæƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯¹ç½‘ç»œä¸­çš„ä¸€äº›æƒé‡åšäº†å¾ˆå°çš„æ”¹å˜ï¼Œè¿™äº›å˜åŒ–ä¼šè®©è¾“å‡ºä¹Ÿæœ‰ç›¸åº”å¾ˆå°çš„å˜åŒ–ï¼š

![](http://neuralnetworksanddeeplearning.com/images/tikz8.png)

via [Neural networks and deep learning - chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)

ç„¶åæˆ‘ä»¬æ ¹æ®è¿™äº›å¾®å°çš„å˜åŒ–ï¼Œè·Ÿç›®æ ‡å€¼å¯¹æ¯”ï¼Œçœ‹çœ‹è¯¯å·®æ˜¯å˜å¤§è¿˜æ˜¯å˜å°äº†ï¼Œç„¶åä¸æ–­è°ƒæ•´æƒé‡çš„å€¼ï¼Œæœ€ç»ˆæ‰¾åˆ°æœ€åˆé€‚çš„ w å’Œ bã€‚

``é˜¿ç‰¹``ï¼šé‚£è¦æ€ä¹ˆæ‰¾åˆ°è¿™äº›å€¼å‘¢ï¼Ÿ

``é˜¿æ‰£``ï¼šä¸‹é¢æœ‰è¯·ã€Œæ¢¯åº¦ä¸‹é™ã€ Gradient Descentã€‚

``é˜¿ç‰¹``ï¼šç»ˆäºèƒ½åæ»‘æ»‘æ¢¯äº†â€¦â€¦

``é˜¿æ‰£``ï¼šåè¿™ä¸ªæ»‘æ»‘æ¢¯å¯èƒ½æœ‰ç‚¹æ™• ğŸ˜„ ã€‚æˆ‘å…ˆç»™ä½ æ‰“ä¸ªæ¯”æ–¹ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨ä¸€ä¸ªå±±å³°çš„å±±é¡¶ï¼Œæƒ³ç”¨æœ€å¿«çš„é€Ÿåº¦åˆ°è¾¾å±±è„šã€‚

``é˜¿ç‰¹``ï¼šåç¼†è½¦å¯ä»¥å—ï¼Ÿ

``é˜¿æ‰£``ï¼šç¼†è½¦ï¼Œä¸å­˜åœ¨çš„â€¦â€¦åªèƒ½é èµ°çš„ã€‚è¦å¾€å“ªè¾¹ä¸‹å±±å‘¢ï¼Ÿæˆ‘ä»¬ä¼šé€‰ä¸€ä¸ªçœ‹èµ·æ¥ã€Œä¸‹é™ã€æœ€å¿«çš„è·¯å¾„ï¼š

![](http://7xjpra.com1.z0.glb.clouddn.com/Gradient%20Descent-i1.png)

æœè¿™ä¸ªæ–¹å‘èµ°ä¸€æ®µåï¼Œæˆ‘ä»¬å†çœ‹ä¸‹ä¸€æ­¥å¾€å“ªä¸ªæ–¹å‘èµ°ï¼Œã€Œä¸‹é™ã€æœ€å¿«ã€‚

![](http://7xjpra.com1.z0.glb.clouddn.com/Gradient%20Descent-i2.png)

ä¸€ç›´é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œå°±èƒ½æœ€å¿«çš„é€Ÿåº¦ä¸‹åˆ°å±±è„šã€‚

``é˜¿ç‰¹``ï¼šæ˜¯è¿™ä¹ˆä¸ªé“ç†ã€‚

``é˜¿æ‰£``ï¼šè¿™ä¸ªæ–¹æ³•ï¼Œå°±æ˜¯ã€Œæ¢¯åº¦ä¸‹é™ã€ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­å¾ˆå¸¸è§ã€‚æ‰€è°“ã€Œæ¢¯åº¦ã€ï¼Œå…¶å®æ˜¯æŒ‡ã€Œå˜åŒ–ç‡ã€æˆ–è€…ã€Œå¡åº¦ slopeã€ï¼Œå°±æ˜¯å¤šå˜é‡å‡½æ•°çš„å¯¼æ•°ã€‚

``é˜¿ç‰¹``ï¼šå¯¼æ•°ï¼Ÿï¼ä½ è¯´çš„æ˜¯å¾®ç§¯åˆ†é‡Œé¢é‚£ä¸ªå¯¼æ•°å—ï¼Ÿ â€¦â€¦ ç‘Ÿç‘Ÿå‘æŠ–.gif

``é˜¿æ‰£``ï¼šåˆ«ç´§å¼ ï¼Œå…ˆå¬æˆ‘è®²ï¼Œå›å¿†å›å¿†ã€‚

``é˜¿ç‰¹``ï¼šå¥½å§ã€‚

``é˜¿æ‰£``ï¼šä½ è¿˜è®°å¾—æ€ä¹ˆè¡¨ç¤ºå‡½æ•° f(x) çš„å¯¼æ•°å§ï¼Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯ f'(x) ã€‚

``é˜¿ç‰¹``ï¼šå—¯å—¯ï¼Œè®°å¾—ã€‚

``é˜¿æ‰£``ï¼šæ‰€è°“ã€Œæ¢¯åº¦ã€ï¼Œå…¶å®å°±æ˜¯å‡½æ•°åœ¨æŸä¸€ç‚¹ä¸Šçš„å˜åŒ–ç‡ï¼Œæ ¹æ®å¾®åˆ†çš„çŸ¥è¯†ï¼Œå˜åŒ–ç‡å¯ä»¥é€šè¿‡è¿™ä¸€ç‚¹çš„åˆ‡çº¿æ±‚å¾—ï¼Œè€Œåˆ‡çº¿å…¶å®å°±æ˜¯å‡½æ•°çš„å¯¼æ•°ï¼šf'(x)ã€‚

![](http://7xjpra.com1.z0.glb.clouddn.com/derivative-example.png)

æ¥ï¼Œè·Ÿæˆ‘å¿µä¸€éï¼šæ±‚æ¢¯åº¦ = æ±‚å˜åŒ–ç‡ = æ±‚å¯¼æ•°

``é˜¿ç‰¹``ï¼šæ±‚æ¢¯åº¦ = æ±‚å˜åŒ–ç‡ = æ±‚å¯¼æ•°

``é˜¿æ‰£``ï¼šäº†è§£äº†ã€Œæ¢¯åº¦ã€ï¼Œç„¶åæˆ‘ä»¬æ¥çœ‹çœ‹ã€Œä¸‹é™ã€åˆæ˜¯æ€ä¹ˆå›äº‹ã€‚åˆ‡çº¿ä»£è¡¨å‡½æ•°åœ¨æŸä¸ªç‚¹çš„å˜åŒ–ç‡ã€‚åœ¨ä¸Šé¢è¿™ä¸ªå›¾ä¸­ï¼Œx = 2 ä½ç½®ä¸Šçš„åˆ‡çº¿ï¼Œæ–œç‡æ˜¯ > 1 çš„ã€‚è¯´æ˜å¦‚æœç»§ç»­å¾€ x = 2 çš„å³è¾¹æ»‘å»ï¼Œåœ¨æ›²çº¿ä¸Šçš„å€¼å°±ä¼šå˜å¤§ã€‚æ¯”å¦‚å½“ x = 3 æ—¶ï¼Œy = 9ã€‚ä½†æ˜¯æˆ‘ä»¬æƒ³è¦åˆ°æ›²çº¿æœ€ä½çš„åœ°æ–¹å»ï¼Œå› ä¸ºé‚£é‡Œå¯ä»¥è®©è¯¯å·®ï¼ˆä¹Ÿå°±æ˜¯ cost ï¼‰æœ€å°ã€‚æ‰€ä»¥ï¼Œåº”è¯¥æ²¿ç€æ¢¯åº¦ã€Œç›¸åã€çš„æ–¹å‘æ»‘åŠ¨ï¼Œä¹Ÿå°±æœç€æ˜¯ x = 2 çš„å·¦è¾¹æ»‘å»ã€‚è¿™å°±æ˜¯ã€Œä¸‹é™ã€çš„å«ä¹‰ã€‚

``é˜¿ç‰¹``ï¼šæ²¿ç€ã€Œä¸Šå±±ã€æœ€å¿«çš„åæ–¹å‘èµ°ï¼Œå°±èƒ½æœ€å¿«ã€Œä¸‹å±±ã€ã€‚å•ŠåŸæ¥è¿™ä¹ˆç›´ç™½â€¦â€¦

``é˜¿æ‰£``ï¼šå¯¹å‘€ï¼ŒåŸç†å¹¶ä¸å¤æ‚çš„ã€‚

è¿™ä¸ªè§†é¢‘è®²è§£äº†çº¿æ€§å›å½’å’Œæ¢¯åº¦ä¸‹é™çš„å…³ç³»ï¼Œæ¥çœ‹çœ‹å§ï¼

[Linear Regression Answer - YouTube](https://www.youtube.com/watch?time_continue=194&v=L5QBqYDNJn0)

``é˜¿ç‰¹``ï¼šå¥½ï¼æœ€å–œæ¬¢çœ‹è§†é¢‘äº†~

``é˜¿æ‰£``ï¼šæˆ‘ä»¬æ¥å¤ä¹ ä¸€ä¸‹ã€‚ç”¨ä¸€ä¸ªå‡½æ•° f(h) è¡¨ç¤º x å’Œ y çš„å…³ç³»ã€‚x å’Œ y å…¶å®æ˜¯å·²çŸ¥çš„ï¼Œå®ƒä»¬æ¥è‡ªçœŸå®çš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¦æ±‚çš„æ˜¯ w å’Œ bï¼Œä½¿å¾—æˆ‘ä»¬è®¡ç®—å‡ºæ¥çš„ $\hat y$ æœ€æ¥è¿‘å®é™…çš„ y å€¼ã€‚ä¸ºäº†å¾—åˆ°æŸç§ç±»å‹çš„ y å€¼ï¼ˆæ¯”å¦‚åªæœ‰ 0 å’Œ 1 ä¸¤ç§è¾“å‡ºï¼‰ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ç±»ä¼¼ Sigmoid è¿™æ ·çš„æ¿€æ´»å‡½æ•°ï¼Œå¯¹ f(h) åšä¸€ä¸‹è½¬æ¢ã€‚

![](http://7xjpra.com1.z0.glb.clouddn.com/simpleNN.png)

``é˜¿ç‰¹``ï¼šå“¦ï¼Œæˆ‘è¯´æ€ä¹ˆæœ‰ç‚¹éš¾ç†è§£å‘¢ã€‚å› ä¸ºä»¥å‰ç¢°åˆ° x å’Œ yï¼Œå®ƒä»¬éƒ½æ˜¯æœªçŸ¥æ•°ï¼Œç°åœ¨å®ƒä»¬å˜æˆäº†å·²çŸ¥æ•°ï¼ŒçœŸæ­£çš„ç›®æ ‡å…¶å®æ˜¯æ±‚ w å’Œ bï¼

``é˜¿æ‰£``ï¼šæ²¡é”™ï¼è¿™æ˜¯æ·±åº¦å­¦ä¹ ç®—æ³•ä¸­ä¸€ä¸ªéœ€è¦ä½ è°ƒæ•´çš„è®¤çŸ¥ã€‚

æ€ä¹ˆå¾—åˆ° w å’Œ b å‘¢ï¼Ÿç”¨æŸå¤±å‡½æ•°ã€‚å¦‚æœæŸå¤±å‡½æ•°çš„å€¼å¤§ï¼Œè¯´æ˜æ¨¡å‹é¢„æµ‹å¾—ä¸å‡†ã€‚å¯ä»¥è®©æŸå¤±å‡½æ•°çš„å€¼æœ€å°çš„ w å’Œ bï¼Œå°±æ˜¯æˆ‘ä»¬è¦æ±‚çš„ã€‚æ›´å…·ä½“è¯´ï¼Œæˆ‘ä»¬è¦æ‰¾åˆ° w çš„å˜åŒ–å¹…åº¦ $\Delta w$ï¼Œæ¯æ¬¡è°ƒæ•´ä¸€å°æ­¥ï¼Œçœ‹çœ‹è¯¯å·® E æ˜¯ä¸æ˜¯å˜å°äº†ã€‚

![](http://7xjpra.com1.z0.glb.clouddn.com/Gradient%20Descent-i3.png)

ä¸ºäº†æ±‚å‡º $\Delta w$ï¼Œæˆ‘ä»¬å¼•å…¥ã€Œè¯¯å·®é¡¹ã€$\delta$ ï¼Œå®ƒè¡¨ç¤º ``è¯¯å·® * æ¿€æ´»å‡½æ•°çš„å¯¼æ•°``ã€‚ç„¶åç”¨ã€Œè¯¯å·®é¡¹ã€$\delta$ ä¹˜ä¸Šå­¦ä¹ ç‡ $\eta$ ï¼ˆç”¨æ¥è°ƒæ•´æ¢¯åº¦çš„å¤§å°ï¼‰ï¼Œå†ä¹˜ä¸Š xï¼Œå°±æ˜¯æ¯æ¬¡åº”è¯¥è°ƒæ•´çš„æƒé‡å€¼ $\Delta w_{ij}$

![](http://7xjpra.com1.z0.glb.clouddn.com/WX20171127-154242@2x.png)

``é˜¿æ‰£``ï¼šæ¯”å¦‚è¯´ï¼Œå¦‚æœæ¿€æ´»å‡½æ•°æ˜¯ Sigmoid å‡½æ•°ã€‚

$$ f(h)=\frac {1}{1 + e^{âˆ’h}}
\\
f'(h)=f(h)(1âˆ’f(h))
\\
\Delta w_{ij}=\eta*(y_j-\hat y_j)*f(h)*(1âˆ’f(h))*x_i $$

â€¦â€¦ å’¦ï¼Ÿäººå‘¢ï¼Ÿ

å–‚ï¼åˆ«è·‘ï¼Œè¿˜æœ‰å¥½å‡ ä¸ªçŸ¥è¯†ç‚¹æ²¡è®²å‘¢ï¼â€¦â€¦

### è¡¥å……1ï¼šæ±‚å¤šä¸ªå˜é‡çš„åå¯¼æ•°

å¦‚æœåªæœ‰ä¸€ä¸ªæœªçŸ¥æ•°ï¼Œæ±‚æ¢¯åº¦åªéœ€è¦è®¡ç®—å¯¼æ•°ã€‚å¦‚æœæœ‰å¤šä¸ªå˜é‡ï¼Œæ±‚æ¢¯åº¦å°±éœ€è¦è®¡ç®—åå¯¼æ•°ã€‚åå¯¼æ•°å…¶å®å¹¶ä¸å¤æ‚ï¼Œåªéœ€è¦æŒæ¡é“¾å¼æ±‚å¯¼æ³•åˆ™ï¼Œå°±èƒ½è¿›è¡Œå¤§éƒ¨åˆ†çš„è®¡ç®—ã€‚

æ¯”å¦‚ï¼ŒæŸå¤±å‡½æ•° C

$$ C = \sum(wx + b - y)^2 = \sum((wx + b)^2 + y^2 - 2y(wx + b))

\\ = \sum(x^2w^2 + b^2 + 2xwb + y^2 - 2xyw - 2yb) $$

å¯¹ w æ±‚åå¯¼

$$ \frac{\partial C}{\partial w} = \frac{1}{N} \sum(wx + b - y)(x) $$

å¯¹ b æ±‚åå¯¼

$$ \frac{\partial C}{\partial b} = \frac{1}{N} \sum(wx + b - y) $$

### è¡¥å……2ï¼šç”¨ python å®ç°æ¢¯åº¦ä¸‹é™çš„ç®—æ³•

```python
# Defining the sigmoid function for activations
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of the sigmoid function
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Input data
x = np.array([0.1, 0.3])
# Target
y = 0.2
# Input to output weights
weights = np.array([-0.8, 0.5])

# The learning rate, eta in the weight step equation
learnrate = 0.5

# the linear combination performed by the node (h in f(h) and f'(h))
h = x[0] * weights[0] + x[1] * weights[1]
# or h = np.dot(x, weights)

# The neural network output (y-hat)
nn_output = sigmoid(h)

# output error (y - y-hat)
error = y - nn_output

# output gradient (f'(h))
output_grad = sigmoid_prime(h)

# error term (lowercase delta)
error_term = error * output_grad

# Gradient descent step
del_w = [ learnrate * error_term * x[0],
          learnrate * error_term * x[1]]
# or del_w = learnrate * error_term * x
```

### Ref
- [Deep Learning Nanodegree | Udacity](https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101)
- [Neural Networks and Deep Learning | Coursera](https://www.coursera.org/learn/neural-networks-deep-learning)
- [Gradient Descent with Squared Errors](https://classroom.udacity.com/nanodegrees/nd101-cn/parts/ba124b66-b7f7-43ab-bc89-a390adb57f92/modules/2afd43e6-f4ce-4849-bde6-49d7164da71b/lessons/dc37fa92-75fd-4d41-b23e-9659dde80866/concepts/7d480208-0453-4457-97c3-56c720c23a89)
- [Gradient (video) | Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)
- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html#momentum)
